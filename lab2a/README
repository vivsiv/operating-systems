Vivek Sivakumar
UID: 303652195

2.1.1 - Causing Conflicts:
Why does it take many iterations before errors are seen?
Errors occur when multiple threads attempt to edit critical section (the add function) at the same time. Its a matter of chance,
the more times you iterate with multiple threads the greater chance you have of having multiple threads entering the critical section
at the same time and producing a synchronization error. If you run too few iterations its possible threads can finish executing before
then next thread is spawned, thus eliminating the possibility of seeing errors.

Why does a significantly smaller number of iterations so seldom fail?
As mentioned in 2.1.1 the code iterates from 1 to n_threads calling pthread_create, since this is done sequentially each thread is already
running before the one after it gets created. If you have a very small number of iterations the previous thread can just finish executing
before the next thread starts, eliminating the possibility of errors.

2.1.2 - cost of yielding:
Why are the --yield runs so much slower?
Yield runs give up the CPU for another thread to execute, forcing a context switch. When we don't use yield context switching is handled and optimized by the OS. However when we introduce yields we add more context switches, decreasing performance and also force context switching
in an unoptimized way when compared to the OS scheduler.

Where is the additional time going?  
The additional time is going into the additional context switches that need to be performed by the OS.

Is it possible to get valid per-operation timings if we are using the --yield option?  If so, explain how.  If not, explain why not.
It is not possible to get valid per-operation timings because the threads are yielding to other threads within their time slice
causing context switches to take up some of the execution time. The calculation of execution time will incorporate the time taken
to perform these switches, ultimately giving us an invalid result.


QUESTION 2.1.3 - measurement errors:
Why does the average cost per operation drop with increasing iterations?
To create threads there is a constant overhead of setting up the threads stack, registers, etc. that will be the same no matter how many
iterations each thread performs. If we perform very few iterations then the thread creation time tends to dominate the overall execution
time meaning a higher avg cost per operation. However if we iterate many times then the thread creation time becomes insignificant compared
to our total execution time and the cost per operation drops, approaching its 'true' value.

If the cost per iteration is a function of the number of iterations, how do we know how many iterations to run (or what the “correct” cost is)?
Building off the answer from 2.1.3, the avg cost per operation cannot drop forever or it would become 0 which is not possible. What actually happens is as the overall execution time increases it dwarfs the thread creation time and the avg cost per operation levels out to a steady value. If we graph the avg cost per exection vs the number of iterations we should be able to see this lower limit (or true value) of avg execution time.

QUESTION 2.1.4 - costs of serialization:
Why do all of the options perform similarly for low numbers of threads?
At low numbers of threads it's tough to see the operational differences between the operations, and the overhead of setting up the protections
tends to dominate (which is somewhat similar across all of the )


Why do the three protected operations slow down as the number of threads rises?
As the number of threads rises the contention for resource protecting the critical section increases so the operations slow down across the board. When there's no protection any thread can execute code during a time slice, but when there's protection a thread needs to wait for 
any threads that are in the critical section to finish before they can execute, resulting in longer execution times.


Why are spin-locks so expensive for large numbers of threads?
Spin locks are very expensive as the number of threads increases because each thread competing for the lock wastes a time slice
spinning. If there are just 2 threads this is not a problem as the thread with the lock will do the work and the thread without will spin and waste its slice. If there are 12 threads then 11 threads without the lock are spinning and wasting slices while the lone thread with the lock is getting time slices less frequently to do work that might actually release the lock.

QUESTION 2.2.1 - scalability of Mutex
Compare the variation in time per protected operation vs the number of threads (for mutex-protected operations) in Part-1 and Part-2, commenting on similarities/differences and offering explanations for them.
The cost of mutexes increases more or less linearly with the number of threads for both add operations and linked list operations which
makes sense because more threads contesting for the lock increases cost for both applications. The raw cost of using mutexes is much higher
for linked list operations than for add operations which makes sense because the linked list operations that are being locked require more
time as the size of the list grows. Interestingly the length adjusted cost was much lower for linked list operations than for add operations,
I wasn't exactly sure why this is as I would expect both to be similar. One possible explanation is that the raw cost of mutexes on linked list operations doesn't begin to grow linearly with the number of iterations until after 1000 iterations and these tests were done using 1000
iterations so divding the cost by 4 might be underestimating the true serialization cost.


QUESTION 2.2.2 - scalability of spin locks
Compare the variation in time per protected operation vs the number of threads for Mutex vs Spin locks, commenting on similarities/differences and offering explanations for them.
Mutexes and spin locks appear to have very similar costs per thread going up to 32 threads, in fact spin locks managed to outperform mutexes
up to 32 threads which is very suprising because high numbers of threads are where spin lock peformance degrades rapidly. My instint is that
1000 iterations is not enough to see the rapid degradation in spinlock performance because even in part 1 the two had very similar performance
numbers until they ventured apart at 16 threads. I'm also not sure about the lnxsrv09 server's specs, but if it is a multi-core processor this would explain the relative efficiency of spinlocks as they thrive more on multi-core processors.